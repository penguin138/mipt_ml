{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МФТИ ФИВТ: Курс Машинное Обучение (осень, 2016), Арсений Ашуха, ars.ashuha@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Organization Info</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительный материал для выполнения дз**:\n",
    "- Hastie, The Elements of Statistical Learning, https://goo.gl/k3wfEU\n",
    "    - 2.9 Model Selection and the Bias–Variance Tradeoff \n",
    "    - 15 Random Forests\n",
    "- Соколов, Семинары по композиционным методам, https://goo.gl/sn8RyJ\n",
    "- Andrew Ng, Bias vs. Variance, https://goo.gl/1ISZ6Y\n",
    "\n",
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2016_fall <номер_группы> <фамилия>``, к примеру -- ``ML2016_fall 401 ivanov``\n",
    "- Выполненное дз сохраните в файл ``<фамилия>_<группа>_task<номер>.ipynb``, к примеру -- ``ivanov_401_task1.ipynb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2016_fall Question <Содержание вопроса>``\n",
    "\n",
    "--------\n",
    "- **PS1**: Мы используем автоматические фильтры, и просто не найдем ваше дз, если вы неаккуратно его подпишете.\n",
    "- **PS2**: Напоминаем, что дедлайны жесткие, письма пришедшие после, автоматически удаляются =(, чтобы соблазна не было "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы своими словами (загугленный материал надо пересказать), ответ обоснуйте (напишите и ОБЪЯСНИТЕ формулки если потребуется), если не выходит, то вернитесь к лекции или дополнительным материалам:\n",
    "\n",
    "**Вопрос 1**: Какие формулы у шума, смещения, разброса? Какой смысл у этих компонент?\n",
    "\n",
    " Шум: $E_{x,y}(a^*(x) - y)^2$ -- показывает насколько в среднем ошибается идеальный алгоритм на наших данных, то есть другими словами показывает степень зашумленности наших данных, наличие в них ошибок.\n",
    "\n",
    " Смещение: $E_{x,y}(a^*(x) - \\bar{a}(x))^2$ -- показывает отклонение \"среднего\" алгоритма $\\bar{a}(x)$ от идеального $a^*(x)$ в среднем по всем объектам. Под средним алгоритмом мы понимаем алгоритм, который на объекте x выдает $E_{X^{l}}\\mu({X^{l}})(x)$, то есть среднее по  ответам алгоритмов, обученных на всевозможных обучающих выборках.\n",
    " \n",
    " Разброс: $E_{x,y}E_{X^{l}}(\\mu({X^{l}})(x) - \\bar{a}(x))^2$ -- это усредненная по всем объектам дисперсия ответов алгоритмов, обученных на всевозможных обучающих выборках, на одном объекте. \n",
    "\n",
    "**Вопрос 2**: 4. Приведите пример семейства алгоритмов с маленьким смещением и большим разбросом. Приведите пример семейства с большим смещением и маленьким разбросом.\n",
    "\n",
    "Решающие деревья имеют маленькое смещение, поскольку могут точно восстановить сложную зависимость, но при этом дерево очень сильно меняется в зависимости от обучающей выборки, следовательно ответы разных деревьев будут сильно отличаться от среднего, следовательно будет большой разброс. \n",
    "Линейные модели имеют большое смещение, потому что линейной моделью сложно хорошо приблизить сложную зависимость, для этого нужно хорошо подбирать нелинейные фичи, но в тоже время линейная модель не будет сильно меняться в зависимости от выборки, поэтому юудет маленький разброс.\n",
    "\n",
    "**Вопрос 3**: Как сгенерировать подвыборку с помощью бутстрапа\n",
    "\n",
    "Бутстрап -- это просто выбор с возвращением из имеющейся выборки. \n",
    "\n",
    "**Вопрос 4**: Что такое бэггинг?\n",
    "\n",
    "Это композиционный метод, при котором набор алгоритмов обучается на случайных подвыборках исходной обучающей выборки, сгенерированных с помощью бутстрапа. Ответ на объекте в данном методе выдается как среднее по ответам каждого из обученных алгоритмов при регрессии или взвешенным голосованием при классификации.\n",
    "\n",
    "**Вопрос 5**:  Как соотносятся смещение и разброс композиции, построенной с помощью бэггинга, со смещением и разбросом одного базового алгоритма\n",
    "\n",
    "Смещение совпадает со смещением одного алгоритма, а разброс зависит от усредненной дисперсии, умноженной на $\\frac{1}{N}$ и усредненной ковариации между алгоритмами, умноженной на  $\\frac{N - 1}{N}$, поэтому если алгоритмы слабо коррелированы, то чем их больше, тем сильнее уменьшится разброс по сравнению с разбросом одного алгоритма.\n",
    "\n",
    "\n",
    "**Вопрос 6**: Как обучается случайный лес? В чем отличия от обычной процедуры построения решающих деревьев?\n",
    "\n",
    "Случайный лес -- это бэггинг для решающих деревьев, то есть каждое дерево обучается на случайной подвыборке. Также иногда в каждой вершине строящегося дерева фичи выбирают не из всех возможных, а из некоторого случайного подмножества, выбираемого в этой вершине.\n",
    "\n",
    "**Вопрос 7**: Почему хорошими базовыми алгоритмами для бэггинга являются именно деревья?\n",
    "\n",
    "Потому что они очень сильно меняются при изменении обучающей выборки (меняются правила в вершинах), поэтому скорее всего деревья в бэггинге будут слабо кореллированы, что будет хорошо уменьшать разброс, особенно если их будет много, оставляя смещение маленьким.\n",
    "\n",
    "**Вопрос 8**: Как оценить качество случайного леса с помощью out-of-bag-процедуры?\n",
    "\n",
    "Нужно считать среднюю ошибку на элементе только по тем деревьям, у которых его не было в обучающей выборке.\n",
    "\n",
    "-----------\n",
    "PS: Если проверяющий не понял ответ на большинство вопросов, то будет пичалька. Пишите так, чтобы можно было разобраться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Bagging</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Известно, что бэггинг плохо работает, если в качестве базовых классификаторов взять knn. Попробуем понять причины на простом примере.\n",
    "\n",
    "Пусть дана выборка $X^l$ из $l$ объектов с ответами из множества $Y = \\{−1, +1\\}$. Будем рассматривать классификатор одного ближайшего соседа в качестве базового алгоритма. Построим с помощью бэггинга композицию длины $N$:\n",
    "\n",
    "$$a_N(x) = sign(\\sum_{n=1}^{N} b_n(x))$$\n",
    "\n",
    "Оцените вероятность того, что ответ композиции на произвольном объекте x будет\n",
    "отличаться от ответа одного классификатора ближайшего соседа, обученного по всей\n",
    "выборке. Покажите, что эта вероятность стремится к нулю при N → ∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**<Решение>**\n",
    "\n",
    "Рассмотрим вероятностное пространство всевозможных выборок длины $l$, полученных из $X^l$ выбором с повторениями.\n",
    "Пусть ответ нашего классификатора не совпадает с ответом композиции на объекте x, а его ближайшим соседом среди всех элементов $X^l$ \n",
    "является z. Это означает, что z не содержится в хотя бы $[\\frac{N}{2}] + 1$ выборках, выбранных для обучения композиции, иначе ответы у хотя бы $[\\frac{N}{2}] + 1$ алгоритмов в композиции совпали бы с ответом классификатора и ответ композиции бы тоже совпадал.\n",
    "\n",
    "Тогда имеем из свойств вероятностной меры, что:\n",
    "\n",
    "искомая вероятность $\\leq P(z\\ не\\ содержится\\ в\\ [\\frac{N}{2}] + 1\\ выборках) = ((1-\\frac{1}{l})^l)^{[\\frac{N}{2}] + 1}$ → 0 при N → ∞ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Bagging Implementation</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте беггинг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from scipy.stats import mode\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class BaggingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, n_estimators, items_rate=1.0, features_rate=1.0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator: sklearn.Classifier\n",
    "            Базовый алгоритм, который можно обучить (есть метод fit).\n",
    "            Для обучения композиции нужно много таких, можно получить с помощю copy.deepcopy\n",
    "\n",
    "        n_estimators: int\n",
    "            Число алгоритмов в композиции\n",
    "\n",
    "        items_rate: float > 0\n",
    "            Доля объектов из трейна, на которой будет обучаться каждый базовый алгоритм\n",
    "\n",
    "        features_rate: float > 0\n",
    "            Доля фичей, на которой будет обучаться и применяться каждый базовый алгоритм\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.items_rate = items_rate\n",
    "        self.features_rate = features_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Метод должен обучить композицию алгоритмов, используя X, y как обучающую выборку.\n",
    "        Не забудьте реализовать функционал выбора случайных объектов и фичей.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array\n",
    "        y: 1d np.array\n",
    "        \"\"\"\n",
    "\n",
    "        # Тут храните обученные базовые алгоритмы\n",
    "        self.estimators = []\n",
    "\n",
    "        # Тут храните фичи для каждого алгоритма\n",
    "        self.features_idx = []\n",
    "        X_len = X.shape[0]\n",
    "        features_len = X.shape[1]\n",
    "        n_features = int(features_len * self.features_rate) + 1\n",
    "        train_size = int(X_len * self.items_rate) + 1\n",
    "        for i in xrange(self.n_estimators):\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            indices = np.random.choice(X_len, size=train_size)\n",
    "            features = np.random.choice(features_len, size=n_features)\n",
    "            self.features_idx.append(features)\n",
    "            estimator = estimator.fit(X[np.ix_(indices, features)], y[indices])\n",
    "            self.estimators.append(estimator)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array матрица объекты признаки на которых нужно сказать ответ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: 1d np.array, Вектор классов для каждого объекта\n",
    "        \"\"\"\n",
    "        \n",
    "        probs_all = [] # Храните тут ответы каждого базового алгоритма \n",
    "        pred_len = X.shape[0]\n",
    "        for i in range(self.n_estimators):\n",
    "            probs = self.estimators[i].predict_proba(X[np.ix_(np.arange(pred_len), self.features_idx[i])])\n",
    "            probs_all.append(probs)\n",
    "        y_pred = []\n",
    "        probs_len = len(probs_all[0][0])\n",
    "        for j in xrange(pred_len):\n",
    "            prob_sums = np.zeros(probs_len)\n",
    "            for i in xrange(self.n_estimators):\n",
    "                prob_sums += np.array(probs_all[i][j])\n",
    "            prob_sums /= self.n_estimators\n",
    "            y_pred.append(np.argmax(prob_sums))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "titanic = pd.read_csv('./data/train.csv')[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']]\n",
    "\n",
    "sex_encoder = LabelEncoder()\n",
    "titanic.Sex = sex_encoder.fit_transform(titanic.Sex)\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = titanic[features].values, titanic.Survived.values\n",
    "X = np.nan_to_num(X)\n",
    "X_train, y_train, X_test, y_test = X[:500], y[:500], X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно обучить свой беггинг на датасете титаник, и посмотреть работает ли он. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.926 0.813299232737\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 10 моделями\n",
    "# =======================================\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(), 10)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите эксперименты:\n",
    "    - Работает ли беггинг лучше, чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cv score: 0.83\n",
      "Best items_rate: 0.45, best features_rate: 1.0\n",
      "Train score: 0.92\n",
      "Test score: 0.81074168798\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 100 моделями\n",
    "# =======================================\n",
    "item_rates = np.arange(0.1, 1.05, 0.05)\n",
    "feature_rates = np.arange(0.1, 1.05, 0.05)\n",
    "model = BaggingClassifier(DecisionTreeClassifier(), 100)\n",
    "grid = GridSearchCV(estimator=model, n_jobs=4, param_grid={'items_rate': item_rates, 'features_rate': feature_rates})\n",
    "grid.fit(X_train, y_train)\n",
    "# summarize the results of the grid search\n",
    "print(\"Best cv score: {}\".format(grid.best_score_))\n",
    "print(\"Best items_rate: {}, best features_rate: {}\".format(grid.best_estimator_.items_rate,\n",
    "                                                           grid.best_estimator_.features_rate))\n",
    "print(\"Train score: {}\".format(accuracy_score(grid.best_estimator_.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(grid.best_estimator_.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8\n",
      "Test score: 0.769820971867\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите LogisticRegression \n",
    "# =======================================\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)\n",
    "print(\"Train score: {}\".format(accuracy_score(model.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(model.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, бэггинг работает лучше. Лучшие параметры указаны выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\n",
    "    './data/adult.data', \n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"], \n",
    "    header=None, na_values=\"?\")\n",
    "\n",
    "adult = pd.get_dummies(adult)\n",
    "adult[\"Target\"] = adult[\"Target_ >50K\"]\n",
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "X_train, y_train, X_test, y_test = X[:20000], y[:20000], X[20000:], y[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы:\n",
    "    - Работает ли беггинг лучше чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cv score: 0.8628\n",
      "Best items_rate: 0.7,  best features_rate: 0.7\n",
      "Train score: 0.9829\n",
      "Test score: 0.862988615556\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier bagging\n",
    "item_rates = np.arange(0.1, 1.1, 0.1)\n",
    "feature_rates = np.arange(0.1, 1.1, 0.1)\n",
    "model = BaggingClassifier(DecisionTreeClassifier(), 100)\n",
    "grid = GridSearchCV(estimator=model, n_jobs=4, param_grid={'items_rate': item_rates, 'features_rate': feature_rates})\n",
    "grid.fit(X_train, y_train)\n",
    "# summarize the results of the grid search\n",
    "print(\"Best cv score: {}\".format(grid.best_score_))\n",
    "print(\"Best items_rate: {},  best features_rate: {}\".format(grid.best_estimator_.items_rate,\n",
    "                                                            grid.best_estimator_.features_rate))\n",
    "print(\"Train score: {}\".format(accuracy_score(grid.best_estimator_.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(grid.best_estimator_.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8012\n",
      "Test score: 0.796353793488\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X_train, y_train)\n",
    "print(\"Train score: {}\".format(accuracy_score(model.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(model.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, бэггинг работает лучше. Лучшие параметры указаны выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Text, Image Classification</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше в каждом эксперименте нужно: \n",
    "- сравниться с линейной моделью ( какую лучше выбрать?=) )\n",
    "- сделать выбор в пользу одной из моделей\n",
    "- выбор обосновать, почему одна из моделей хуже а другая лучше\n",
    "- что такое хуже и лучше\n",
    "- попробуйте беггинг над деревьями и линейными моделями \n",
    "- почему работает или не работает, какие особенности данных на это влияют"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train, y_train = vectorizer.fit_transform(newsgroups_train.data), newsgroups_train.target\n",
    "X_test,  y_test  = vectorizer.transform(newsgroups_test.data), newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку мы имеем дело с текстами, то пространство у нас очень большой размерности, поэтому в качестве линейной модели будем использовать SVM. \n",
    "\n",
    "В sklearn возьмем SGDClassifier, в котором hinge loss стоит по дефолту. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.992929114372\n",
      "Test score: 0.850106213489\n",
      "CPU times: user 1.54 s, sys: 29 ms, total: 1.57 s\n",
      "Wall time: 823 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# =======================================\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"Train score: {}\".format(accuracy_score(clf.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# =======================================\n",
    "\n",
    "model = BaggingClassifier(DecisionTreeClassifier(), 50, items_rate=0.5, features_rate=0.5)\n",
    "model = model.fit(X_train, y_train)\n",
    "print(\"Train score: {}\".format(accuracy_score(model.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(model.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае линейный классификатор (85% accuracy на тесте) дает результат лучше, чем bagging (35% accuracy на тесте), потому что bagging только на 10 деревьях. \n",
    "Скорее всего на 50 деревьях результат был бы лучше, но я не смогла дождаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import load_cifar10\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10('./data/cifar10')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(X_train.shape[0], -1), X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.50944\n",
      "Test score: 0.3637\n",
      "CPU times: user 14.9 s, sys: 645 ms, total: 15.5 s\n",
      "Wall time: 8.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# =======================================\n",
    "clf = RidgeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(\"Train score: {}\".format(accuracy_score(clf.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(clf.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9072\n",
      "Test score: 0.3568\n",
      "CPU times: user 6min 42s, sys: 7.06 s, total: 6min 49s\n",
      "Wall time: 6min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# =======================================\n",
    "model = BaggingClassifier(DecisionTreeClassifier(), 50, items_rate=0.5, features_rate=0.5)\n",
    "model = model.fit(X_train, y_train)\n",
    "print(\"Train score: {}\".format(accuracy_score(model.predict(X_train), y_train)))\n",
    "print(\"Test score: {}\".format(accuracy_score(model.predict(X_test), y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае оба классификатора дают примеро одинаковый плохой результат, но тут видно, что bagging переобучается, а линейная модель нет, поэтому линейная модель тут лучше.\n",
    "Скорее всего на 50 деревьях результат был бы лучше, но я не смогла дождаться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Random Forest Feature Importance</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Опишите как вычисляется важность фичей в дереве, можете изучить как работает  feature\\_importances_ в sklearn.\n",
    "\n",
    "---\n",
    "В sklearn feature_importances считаются следующим образом: для каждого признака берутся вершины, в которых разбиение идет по этому признаку, для них считается уменьшение значения критерия Джини до и после сплита, затем все эти значения складываются по всем деревьям в лесу и нормируются на количество деревьев. Чем больше такое суммарное уменьшение критерия Джини, тем важнее признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посчитайте Feature Impotance для Adult и Titanic (используйте полный датасет), ПРОИНТЕРПРЕТИРУЙТЕ результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "X_train, y_train, X_test, y_test = X[:20000], y[:20000], X[20000:], y[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n",
    "                        items_rate=1, features_rate=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.34920216e-01   2.09017925e-01   8.58732953e-02   1.04322841e-01\n",
      "   3.83045969e-02   6.55744307e-02   2.46841861e-03   5.95766112e-03\n",
      "   8.11260499e-03   0.00000000e+00   1.49892694e-02   8.09581058e-03\n",
      "   1.14098439e-02   5.44981935e-03   6.46946663e-06   1.89076215e-03\n",
      "   2.18377916e-03   1.07919402e-03   1.90754974e-04   7.35556612e-04\n",
      "   1.97680755e-03   9.89893732e-04   3.29716583e-03   4.62824378e-03\n",
      "   8.21573585e-03   3.86111584e-03   7.09098224e-03   8.67628734e-03\n",
      "   1.56732802e-04   2.24943648e-03   7.06287427e-03   6.62452199e-03\n",
      "   5.32363253e-04   1.86526489e-01   1.28970323e-03   9.49280708e-03\n",
      "   2.46745849e-03   1.64316047e-03   2.49053627e-03   7.17342564e-03\n",
      "   1.41806594e-05   9.64348155e-03   1.90171955e-02   4.54041050e-03\n",
      "   3.06557439e-03   5.60916895e-03   6.19664140e-03   8.05809002e-05\n",
      "   1.56679737e-02   4.17874316e-03   1.03523585e-02   5.70251642e-03\n",
      "   5.92832214e-03   5.22419107e-02   6.13466808e-03   2.25132589e-03\n",
      "   4.14027364e-03   3.58351701e-03   1.21913160e-02   1.44105648e-03\n",
      "   3.35867111e-03   5.73741829e-03   1.26809947e-03   7.98230265e-03\n",
      "   6.68023524e-03   5.78586790e-03   3.48848924e-03   2.93285590e-04\n",
      "   1.40939787e-03   4.67630889e-04   2.25590129e-04   8.23410004e-04\n",
      "   2.05177819e-04   2.05496271e-04   2.18412553e-04   1.00433175e-03\n",
      "   1.59922408e-04   1.40739717e-03   2.90758599e-04   7.61270086e-05\n",
      "   9.04704260e-05   0.00000000e+00   7.83865129e-06   1.30100331e-04\n",
      "   1.14542522e-04   1.31869240e-03   5.98542216e-04   2.70532380e-04\n",
      "   1.05352663e-03   5.30552872e-04   9.09918282e-04   6.21362331e-06\n",
      "   1.88806041e-03   1.38385114e-04   9.81655139e-06   3.00112185e-05\n",
      "   1.05184338e-03   8.28025578e-04   2.70367172e-04   8.01032045e-04\n",
      "   2.10428365e-05   7.45309166e-04   3.85836968e-04   8.21893732e-05\n",
      "   4.14012384e-05   7.46731922e-03   3.00647883e-04   4.57662510e-04]\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Посчитайте feature_importances для clf\n",
    "# =======================================\n",
    "n_features = X.shape[1]\n",
    "feature_importances = np.zeros(n_features)\n",
    "feature_counts = np.zeros(n_features)\n",
    "for i in xrange(clf.n_estimators):\n",
    "    importances = clf.estimators[i].feature_importances_\n",
    "    feature_importances[clf.features_idx[i]] += importances\n",
    "    feature_counts[clf.features_idx[i]] += np.ones(len(importances)) \n",
    "feature_importances /= feature_counts\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на топ-5 признаков с наибольшим feature_importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'fnlwgt', u'Martial Status_ Married-civ-spouse', u'Age',\n",
      "       u'Capital Gain', u'Education-Num'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "indices = feature_importances.argsort()[-5:][::-1]\n",
    "print(adult.columns[indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вполне логично, что одними из самых важных признаков тут являются возраст и образование, а Capital Gain как мне кажется почти напрямую связана с target_value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = titanic[features].values, titanic.Survived.values\n",
    "X = np.nan_to_num(X)\n",
    "X_train, y_train, X_test, y_test = X[:500], y[:500], X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n",
    "                        items_rate=1, features_rate=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07165623  0.29956385  0.22082358  0.32399596]\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Посчитайте feature_importances для clf\n",
    "# =======================================\n",
    "n_features = X.shape[1]\n",
    "feature_importances = np.zeros(n_features)\n",
    "feature_counts = np.zeros(n_features)\n",
    "for i in xrange(clf.n_estimators):\n",
    "    importances = clf.estimators[i].feature_importances_\n",
    "    feature_importances[clf.features_idx[i]] += importances\n",
    "    feature_counts[clf.features_idx[i]] += np.ones(len(importances)) \n",
    "feature_importances /= feature_counts\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Получается, что выживание пассажира Титаника не зависит от Pclass и примерно одинаково зависит от пола и цены билета, но в меньшей степени от возраста, хотя по идее Pclass и Fare должны быть зависимы между собой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
