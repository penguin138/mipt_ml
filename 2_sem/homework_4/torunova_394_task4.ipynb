{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МФТИ ФИВТ: Курс Машинное Обучение (осень, 2016), Арсений Ашуха, ars.ashuha@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 1**: Зачем нужно структурное предсказание?\n",
    "\n",
    "Оно используется, когда предсказываемые значения для разных объектов связаны между собой, например когда мы хотим предсказывать по пикселю на картинке label объекта, к которому он принадлежит.\n",
    "\n",
    "**Вопрос 2**: Что такое сопряженное распределение?\n",
    "\n",
    "Если апостериорное распределение $p(\\theta | x)$ и априорное распределение $p(\\theta)$ принадлежат к одному семейству распределений, то это семейство называется сопряженным к семейству функций правдоподобия $p(x | \\theta)$.\n",
    "\n",
    "**Вопрос 3**: Какое распределение сопряженное к равномерному, докажите?\n",
    "\n",
    "Это распределение Парето.\n",
    "\n",
    "Пусть $p(\\theta)$ -- плотность распределения Парето c параметрами $\\theta_m$ и $k$, $p(x | \\theta)$ -- плотность равномерного распределения $U[0, \\theta]$. Покажем, что тогда $p(\\theta | x)$ -- тоже плотность распределения Парето.\n",
    "Итак, рассмотрим  $p(\\theta) = \\frac{k\\theta_m^k}{\\theta^{k+1}}\\cdot I(\\theta \\geq \\theta_m)$ и $p(x | \\theta) = \\frac{I(x \\in [0, \\theta])}{\\theta}$. Тогда по формуле Байеса получаем: $p(\\theta | x) = \\frac{p(x | \\theta)p(\\theta)}{\\int \\limits_{\\Theta} p(x | \\theta)p(\\theta)d\\theta} = \\frac{\\frac{k\\theta_m^k}{\\theta^{k+2}}\\cdot I(\\theta \\geq \\theta_m)I(x \\in [0, \\theta])}{\\int \\limits_{\\Theta}\\frac{k\\theta_m^k}{\\theta^{k+2}}\\cdot I(\\theta \\geq \\theta_m)I(x \\in [0, \\theta]) d\\theta } = \\frac{I(\\theta \\geq max(\\theta_m, x))}{\\theta^{k+2}} \\int \\limits_{max(\\theta_m, x)}^{\\infty} \\frac{d\\theta}{\\theta^{k+2}} = \\frac{(k+1)max(\\theta_m, x)^{k+1}I(\\theta \\geq max(\\theta_m, x))}{\\theta^{k+2}}$ -- плотность распределения Парето с параметрами $(k+1)$ и $max(\\theta_m, x)$ \n",
    "\n",
    "**Вопрос 4**: В чем заключается Байесовский подход к машинному обучению?\n",
    "\n",
    "Зависимости между векторами объектов X, значениями целевых переменных y и параметрами модели W моделируются в виде совместного распределения $p(X, y, W)$. В классической постановке $p(X, y, W) = \\prod \\limits_{1}^{N} p(y_i | x_i, W)p(x_i | W)p(W)$ \n",
    "\n",
    "**Вопрос 5**: В чем основные преимущества Байесовского подхода к машинному обучения?\n",
    "\n",
    "Мы можем задавать распределение на веса модели - p(W), что помогает учесть специфику задачи.\n",
    "\n",
    "**Вопрос 6**: Чем отличается структурный метод опорных векторов от неструктурного, в чем сложности при обучении структурного?\n",
    "В обычном SVM мы хотим максимизировать зазор между объектами и разделяющей гиперплоскостью, а задача оптимизации выглядит так:\n",
    "$\\frac{1}{2}||w||^2 + C\\cdot \\sum \\limits_{1}^{N}\\xi_i \\rightarrow min$ при ограничениях $y_i \\cdot w_{t_i} x_i \\geq 1 - \\xi_i$, $\\xi_i > 0$.\n",
    "\n",
    "В Structured SVM мы хотим минимизировать $E(X, y, W) = -\\log p(y | X, W)$ -- парно-сепарабельную энергию. Соответственно задача преобразуется: $\\frac{1}{2}||W||^2 + C\\xi \\rightarrow min$ при ограничениях: $E(X_{tr}, y_{tr}, W) \\leq min[(E(X_{tr}, y, W) - \\delta(y, y_{tr}))] + \\xi$. Cложность в том, что в ограничении одной задачи оптимизации, стоит другая задача оптимизации по экспоненциально большому множеству ответов, но есть метод, гарантирующий сходимость за полиномиальное время. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Task</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу построения коллажа из нескольких изображений, к примеру несколько фотографий группы людей, на каждой кто-то отвернулся или моргнул ... Вы хотите получить одну фотографию, на которой все получились хорошо. На вход поступает K изображений а на выходе вам нужно выдать матрицу размером с изображение, где в каждом пикселе будет указано из какой картинки вам нужно его взять. Вы хотите сделать фотографию так, чтобы места склейки были незаметны. \n",
    "\n",
    "- Введите граф модель, почему вы выбрали именно такую, приложите рисунок\n",
    "- Потенциалы каких порядков вы собираетесь использовать? \n",
    "- Определите потенциалы -- какой в них физический смысл? почему они поощряют незаметные склеивания?\n",
    "- Предложить несколько вариантов выбора потенциалов.\n",
    "\n",
    "![](ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем в качестве граф модели сеточку: вершины это пиксели, они соединены ребрами со своими соседями по стороне. Это имеет смысл, потому что наверняка ответ на пикселе должен зависеть от ответов на его соседях. Вот рисунок:\n",
    "![](http://mathworld.wolfram.com/images/eps-gif/GridGraph_701.gif)\n",
    "Предположим, что никто кроме людей не двигается и картинки выровнены так, что пиксели недвигающихся объектов максимально совпадают. Будем использовать унарные и парные потенциалы. Унарные потенциалы должны в каждом пикселе, в котором есть изменение цвета выдавать 0, если используется самый редкий цвет, среди всех цветов этого пикселя на картинках и 1 иначе. Поскольку на изображениях двигаются только люди, то так мы будем выбирать пиксели с изображения, где человек присутствует. В качестве парных потенциалов можно взять $\\phi(x_i, x_j) = |c(x_i) - c(x_j)| * I(y_i != y_j)$, где -$y_i$ -классы пикселей $x_i$, а $c(x_i)$ -- их цвета. Так будет поощряться одинаковость цветов на границах классов. Еще можно ввести в сетке дополнительные связи, а в потенциале делить на расстояние между пикселями. Расстояния также можно измерять разными метриками - евклидово, манхэттенское итд."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Bonus part: Semantic Image Segmentation</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Если вы, хотите разобраться с граф моделями -- попробуйте реализовать это на питон, выполнение займет около 30-40 часов, на щедрые бонусные баллы (3-5 баллов) -- можно обсудить заранее. Если решитесь, напишите мне. \n",
    "\n",
    "http://www.machinelearning.ru/wiki/index.php?title=Графические_модели_(курс_лекций)/2012/Задание_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
