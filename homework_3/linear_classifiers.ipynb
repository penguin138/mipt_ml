{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном домашнем задании вы будете решать задачу классификации отзывов.\n",
    "\n",
    "Шаги решения:\n",
    "\n",
    "1. Извлечение признаков: напишите код для создания TF-IDF матрицы из представленного корпуса отзывов\n",
    "2. Обучение моделей: напишите код для обучения SVM и логистической регрессии\n",
    "3. Кросс-валидация для подбора гиперпараметров: напишите код для оптимизации метрик обучения\n",
    "4. Участие в контесте на kaggle.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from linear_svm import svm_loss_naive, svm_loss_vectorized\n",
    "from gradient_check import grad_check_sparse\n",
    "from linear_classifier import LinearSVM, Softmax\n",
    "from softmax import softmax_loss_naive, softmax_loss_vectorized\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "#### Знакомство с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(352278, 2)\n",
      "                                                  summary  score\n",
      "id                                                              \n",
      "230872                                  Babies love these      3\n",
      "344823                                       Salmon Trout      0\n",
      "211754                                     disappointment      1\n",
      "259421  Doesn't taste like Cinnabon; tastes like Waffl...      2\n",
      "253418  Delicious San Daniele prosciutto and good cust...      3\n",
      "                                  summary\n",
      "id                                       \n",
      "365507                    CHECK THE SUGAR\n",
      "401398                      Great Product\n",
      "45480                   This stuff rocks!\n",
      "396287                   community coffee\n",
      "44193   Not my favorite but good for you!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('kaggle_data/train.csv', index_col=0, na_values='NaN')\n",
    "test_data = pd.read_csv('kaggle_data/test.csv', index_col=0, na_values='NaN')\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:\n",
      "  max:3\n",
      "  min:0\n",
      "Docs:\n",
      "  Babies love these\n",
      "  Salmon Trout\n",
      "  disappointment\n",
      "  Doesn't taste like Cinnabon; tastes like Waffle Crisp\n",
      "  Delicious San Daniele prosciutto and good customer service\n",
      "  My Dog Loves Them\n",
      "  My husband's new favorite coffee.\n",
      "  Good Job, Betty Crocker\n",
      "  Good chips, more cheese\n",
      "  Nature's Hallow Sugar Free Jam\n"
     ]
    }
   ],
   "source": [
    "documents = data.summary.values\n",
    "score = data.score.values\n",
    "print(\"Score:\\n  max:{}\\n  min:{}\".format(max(score), min(score)))\n",
    "print(\"Docs:\\n  \" + \"\\n  \".join(documents[:10]))\n",
    "test_docs = test_data.summary.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, каждый объект представляет собой отзыв о продукте и оценку по шкале от 0 до 3. Выдвинем гипотезу, что слова, используемые в написании отзыва коррелируют с оценкой, которая была поставлена. Поставим задачу - предсказать оценку по тексту отзыва."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "###  Извлечение признаков\n",
    "\n",
    "Для решения задачи классификации необходимо преобразовать каждый отзыв (документ) в вектор. Размерность данного вектора будет равна количеству слов используемых в корпусе (все документы). Каждая координата соответствует слову, значение в координает равно количеству раз, слово используется в документе.\n",
    "\n",
    "В итоге получится матрица, в (i,j) ячейке которой написано количество раз, которое j-e слово встречается в i-ом документе. Заметим, что у такой матрицы получаются сильно разреженные строки(с большим количеством нулей).\n",
    "\n",
    "Для учета важности редких, но показательных слов (термов), используется схема взвешивания TF-IDF. Преобразуем матрицу частот в матрицу документов, частоты термов которых взвешенны по TF-IDF.\n",
    "\n",
    "Это преобразование можно делать сразу из набора документов с помощью TfidfVectorizer или сначала посчитать матрицу \n",
    "частот с помощью CountVectorizer, а потом преобразовать ее с помощью TfidfTransformer (как изначально было предложено в задании)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя CountVectorizer + TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix2 = tfidf_transformer.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что результат одинаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix - tfidf_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но работать приятнее все-таки с TfidfVectorizer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам придется преобразовать полученную матрицу в numpy array и выполнять его преобразования, поэтому, чтобы python kernel не умирал, сократим максимальное количество слов-признаков каждого документа до 3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_matrix_test = tfidf_vectorizer.fit_transform(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем теперь полученную csr матрицу в numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_matrix.toarray()\n",
    "tfidf_matrix_test = tfidf_matrix_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 2. Код для SVM и логистической регресии - 40 Баллов\n",
    "\n",
    "После того, как вы получили матрицу признаков, вам необходимо реализовать алгоритм обучения SVM и логистической регрессии. Обе модели являются линейными и отличаются функциями потерь. Для решения оптимизационных задач в обеих моделях будет использоваться стохастический градиентный спуск.\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "\n",
    "- Линейные модели: http://cs231n.github.io/linear-classify/\n",
    "- SGD: http://cs231n.github.io/optimization-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с SVM стартовый код находится в файле cs231n/classifiers/linear_svm.py вашей задачей является реализация подсчета функции потерь для SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте обучающую выборку на 2 части train и test\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "- Используйте трансформер: http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Транспонируем матрицы с данными, т.к. так будет проще реализовать код SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = tfidf_matrix.transpose()\n",
    "X_test = tfidf_matrix_test.transpose()\n",
    "y_train = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем подвыборки из обучающей выборки, для быстрой проверки кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_sample = X_train[:, 0:100000]\n",
    "X_test_sample = X_test[:,0:100000]\n",
    "y_train_sample = y_train[0:100000]\n",
    "# bias trick:\n",
    "# temp = np.ones((X_train_sample.shape[0] + 1,X_train_sample.shape[1]))\n",
    "# temp[:-1,:] = X_train_sample\n",
    " #X_train_sample = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем чему равен градиент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.21 s, sys: 901 ms, total: 5.11 s\n",
      "Wall time: 3.88 s\n",
      "loss: 3.0029369998458133\n",
      " gradient:[[ -1.61533995e-05   2.88551922e-04   2.86618161e-04 ...,   9.23480888e-05\n",
      "    2.27148316e-04   2.17642461e-04]\n",
      " [  5.60427920e-05   4.42579413e-04   3.21660025e-04 ...,   6.42338727e-05\n",
      "    1.86188150e-04   2.17398548e-04]\n",
      " [  8.33125612e-06   1.17628876e-04   1.31344553e-04 ...,  -8.03456269e-06\n",
      "    2.27394512e-04   1.17086937e-04]\n",
      " [ -4.81323335e-05  -8.48602959e-04  -7.39709202e-04 ...,  -1.48994468e-04\n",
      "   -6.40664844e-04  -5.52171682e-04]]\n"
     ]
    }
   ],
   "source": [
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(4, X_train_sample.shape[0]) * 0.01 \n",
    "% time loss, grad = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "print('loss: {}\\n gradient:{}'.format(loss, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент равен 0, т.к. код который должен его считать отсутствует. Реализуйте наивную версию и проверьте результат с помощью численного метода расчета. Градиенты должны почти совпадать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000580 analytic: 0.000580, relative error: 4.793090e-08\n",
      "numerical: 0.000949 analytic: 0.000949, relative error: 1.210133e-07\n",
      "numerical: 0.000091 analytic: 0.000091, relative error: 2.042296e-07\n",
      "numerical: -0.000324 analytic: -0.000324, relative error: 4.101075e-08\n",
      "numerical: 0.000295 analytic: 0.000295, relative error: 1.594785e-07\n"
     ]
    }
   ],
   "source": [
    "# Once you've implemented the gradient, recompute it with the code below\n",
    "# and gradient check it with the function we provided for you\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "loss, grad = svm_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should match\n",
    "# almost exactly along all dimensions.\n",
    "f = lambda w: svm_loss_naive(w, X_train_sample, y_train_sample, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуйте векторизованную версию расчета фунции потерь - svm_loss_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 3.002937e+00 computed in 3.101828s\n",
      "Vectorized loss: 3.002937e+00 computed in 0.479060s\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, _ = svm_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be much faster.\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завершите реализацию SVM, реализуйте векторизированную версию расчета градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 3.327640s\n",
      "Vectorized loss and gradient: computed in 0.506977s\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values computed\n",
    "# by the two implementations. The gradient on the other hand is a matrix, so\n",
    "# we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 500: loss 3.000143\n",
      "iteration 100 / 500: loss 2.637392\n"
     ]
    }
   ],
   "source": [
    "# Now implement SGD in LinearSVM.train() function and run it with the code below\n",
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=5e-2, lambda_=0.01,\n",
    "                      num_iters=500, verbose=True, batch_size=20000)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "print('Current loss is %f' % loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function \n",
    "y_train_pred = svm.predict(X_train)\n",
    "# y_test_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#and evaluate the performance on both the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare result with the most common dummy classifier\n",
    "print(classification_report(y_train, [3]*len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.386416\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(4, X_train_sample.shape[0]) * 0.01 \n",
    "loss, grad = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "f = lambda w: softmax_loss_naive(w, X_train_sample, y_train_sample, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 1.386428e+00 computed in 0.521171s\n",
      "vectorized loss: 1.386428e+00 computed in 0.314698s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sm = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = sm.train(X_train, y_train, learning_rate=5e-2, reg=0.01,\n",
    "                      num_iters=500, verbose=True, batch_size=20000)\n",
    "\n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)\n",
    "print 'Current loss is %f' % loss_hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function \n",
    "y_train_pred = sm.predict(X_train)\n",
    "y_test_pred = sm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare result with the most common dummy classifier\n",
    "print classification_report(y_test, [3]*len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 3. Kaggle In Class - 50 Баллов\n",
    "\n",
    "Используйте полученные модели для решения контеста. Выберете одну из моделей, реализуйте настройку гиперпараметров и пайплайн для предсказания классов тестовой выборки для сабмита в систему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 4. Бонусы - 30 Баллов\n",
    "\n",
    "Улучшите результат: \n",
    "\n",
    "- Нормализуйте слова в документах, используйте стоп слова, попробуйте различные стратегии нормализации в TF-IDF\n",
    "- Используйте PCA, Word2Vec для расширения пространства фичей\n",
    "- Придумайте другие стратегии и улучшайте свой результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
