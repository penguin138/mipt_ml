{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном домашнем задании вы будете решать задачу классификации отзывов.\n",
    "\n",
    "Шаги решения:\n",
    "\n",
    "1. Извлечение признаков: напишите код для создания TF-IDF матрицы из представленного корпуса отзывов\n",
    "2. Обучение моделей: напишите код для обучения SVM и логистической регрессии\n",
    "3. Кросс-валидация для подбора гиперпараметров: напишите код для оптимизации метрик обучения\n",
    "4. Участие в контесте на kaggle.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from linear_svm import svm_loss_naive, svm_loss_vectorized\n",
    "from gradient_check import grad_check_sparse\n",
    "from linear_classifier import LinearSVM, Softmax\n",
    "from softmax import softmax_loss_naive, softmax_loss_vectorized\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "#### Знакомство с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(352278, 2)\n",
      "                                                  summary  score\n",
      "id                                                              \n",
      "230872                                  Babies love these      3\n",
      "344823                                       Salmon Trout      0\n",
      "211754                                     disappointment      1\n",
      "259421  Doesn't taste like Cinnabon; tastes like Waffl...      2\n",
      "253418  Delicious San Daniele prosciutto and good cust...      3\n",
      "                                  summary\n",
      "id                                       \n",
      "365507                    CHECK THE SUGAR\n",
      "401398                      Great Product\n",
      "45480                   This stuff rocks!\n",
      "396287                   community coffee\n",
      "44193   Not my favorite but good for you!\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('kaggle_data/train.csv', index_col=0, na_values='NaN')\n",
    "test_data = pd.read_csv('kaggle_data/test.csv', index_col=0, na_values='NaN')\n",
    "print(data.shape)\n",
    "print(data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:\n",
      "  max:3\n",
      "  min:0\n",
      "Docs:\n",
      "  Babies love these\n",
      "  Salmon Trout\n",
      "  disappointment\n",
      "  Doesn't taste like Cinnabon; tastes like Waffle Crisp\n",
      "  Delicious San Daniele prosciutto and good customer service\n",
      "  My Dog Loves Them\n",
      "  My husband's new favorite coffee.\n",
      "  Good Job, Betty Crocker\n",
      "  Good chips, more cheese\n",
      "  Nature's Hallow Sugar Free Jam\n"
     ]
    }
   ],
   "source": [
    "documents = data.summary.values\n",
    "score = data.score.values\n",
    "print(\"Score:\\n  max:{}\\n  min:{}\".format(max(score), min(score)))\n",
    "print(\"Docs:\\n  \" + \"\\n  \".join(documents[:10]))\n",
    "test_docs = test_data.summary.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, каждый объект представляет собой отзыв о продукте и оценку по шкале от 0 до 3. Выдвинем гипотезу, что слова, используемые в написании отзыва коррелируют с оценкой, которая была поставлена. Поставим задачу - предсказать оценку по тексту отзыва."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "###  Извлечение признаков\n",
    "\n",
    "Для решения задачи классификации необходимо преобразовать каждый отзыв (документ) в вектор. Размерность данного вектора будет равна количеству слов используемых в корпусе (все документы). Каждая координата соответствует слову, значение в координает равно количеству раз, слово используется в документе.\n",
    "\n",
    "В итоге получится матрица, в (i,j) ячейке которой написано количество раз, которое j-e слово встречается в i-ом документе. Заметим, что у такой матрицы получаются сильно разреженные строки(с большим количеством нулей).\n",
    "\n",
    "Для учета важности редких, но показательных слов (термов), используется схема взвешивания TF-IDF. Преобразуем матрицу частот в матрицу документов, частоты термов которых взвешенны по TF-IDF.\n",
    "\n",
    "Это преобразование можно делать сразу из набора документов с помощью TfidfVectorizer или сначала посчитать матрицу \n",
    "частот с помощью CountVectorizer, а потом преобразовать ее с помощью TfidfTransformer (как изначально было предложено в задании)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя CountVectorizer + TfidfTransformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_matrix = count_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix2 = tfidf_transformer.fit_transform(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что результат одинаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix - tfidf_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но работать приятнее все-таки с TfidfVectorizer :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам придется преобразовать полученную матрицу в numpy array и выполнять его преобразования, поэтому, чтобы python kernel не умирал, сократим максимальное количество слов-признаков каждого документа до 3000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_matrix_test = tfidf_vectorizer.fit_transform(test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем теперь полученную csr матрицу в numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_matrix.toarray()\n",
    "tfidf_matrix_test = tfidf_matrix_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 2. Код для SVM и логистической регресии - 40 Баллов\n",
    "\n",
    "После того, как вы получили матрицу признаков, вам необходимо реализовать алгоритм обучения SVM и логистической регрессии. Обе модели являются линейными и отличаются функциями потерь. Для решения оптимизационных задач в обеих моделях будет использоваться стохастический градиентный спуск.\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "\n",
    "- Линейные модели: http://cs231n.github.io/linear-classify/\n",
    "- SGD: http://cs231n.github.io/optimization-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с SVM стартовый код находится в файле cs231n/classifiers/linear_svm.py вашей задачей является реализация подсчета функции потерь для SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте обучающую выборку на 2 части train и test\n",
    "\n",
    "Дополнительная информация для решения задачи:\n",
    "- Используйте трансформер: http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Транспонируем матрицы с данными, т.к. так будет проще реализовать код SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = tfidf_matrix.transpose()\n",
    "X_test = tfidf_matrix_test.transpose()\n",
    "y_train = score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем подвыборки из обучающей выборки, для быстрой проверки кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_sample = X_train[:, 0:100000]\n",
    "X_test_sample = X_test[:,0:100000]\n",
    "y_train_sample = y_train[0:100000]\n",
    "# bias trick:\n",
    "# temp = np.ones((X_train_sample.shape[0] + 1,X_train_sample.shape[1]))\n",
    "# temp[:-1,:] = X_train_sample\n",
    " #X_train_sample = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем чему равен градиент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.21 s, sys: 901 ms, total: 5.11 s\n",
      "Wall time: 3.88 s\n",
      "loss: 3.0029369998458133\n",
      " gradient:[[ -1.61533995e-05   2.88551922e-04   2.86618161e-04 ...,   9.23480888e-05\n",
      "    2.27148316e-04   2.17642461e-04]\n",
      " [  5.60427920e-05   4.42579413e-04   3.21660025e-04 ...,   6.42338727e-05\n",
      "    1.86188150e-04   2.17398548e-04]\n",
      " [  8.33125612e-06   1.17628876e-04   1.31344553e-04 ...,  -8.03456269e-06\n",
      "    2.27394512e-04   1.17086937e-04]\n",
      " [ -4.81323335e-05  -8.48602959e-04  -7.39709202e-04 ...,  -1.48994468e-04\n",
      "   -6.40664844e-04  -5.52171682e-04]]\n"
     ]
    }
   ],
   "source": [
    "# generate a random SVM weight matrix of small numbers\n",
    "W = np.random.randn(4, X_train_sample.shape[0]) * 0.01 \n",
    "% time loss, grad = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "print('loss: {}\\n gradient:{}'.format(loss, grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиент равен 0, т.к. код который должен его считать отсутствует. Реализуйте наивную версию и проверьте результат с помощью численного метода расчета. Градиенты должны почти совпадать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000580 analytic: 0.000580, relative error: 4.793090e-08\n",
      "numerical: 0.000949 analytic: 0.000949, relative error: 1.210133e-07\n",
      "numerical: 0.000091 analytic: 0.000091, relative error: 2.042296e-07\n",
      "numerical: -0.000324 analytic: -0.000324, relative error: 4.101075e-08\n",
      "numerical: 0.000295 analytic: 0.000295, relative error: 1.594785e-07\n"
     ]
    }
   ],
   "source": [
    "# Once you've implemented the gradient, recompute it with the code below\n",
    "# and gradient check it with the function we provided for you\n",
    "\n",
    "# Compute the loss and its gradient at W.\n",
    "loss, grad = svm_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# Numerically compute the gradient along several randomly chosen dimensions, and\n",
    "# compare them with your analytically computed gradient. The numbers should match\n",
    "# almost exactly along all dimensions.\n",
    "f = lambda w: svm_loss_naive(w, X_train_sample, y_train_sample, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуйте векторизованную версию расчета фунции потерь - svm_loss_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss: 3.002937e+00 computed in 3.101828s\n",
      "Vectorized loss: 3.002937e+00 computed in 0.479060s\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, _ = svm_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# The losses should match but your vectorized implementation should be much faster.\n",
    "print('difference: %f' % (loss_naive - loss_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Завершите реализацию SVM, реализуйте векторизированную версию расчета градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loss and gradient: computed in 3.327640s\n",
      "Vectorized loss and gradient: computed in 0.506977s\n",
      "difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "_, grad_naive = svm_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Naive loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "_, grad_vectorized = svm_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('Vectorized loss and gradient: computed in %fs' % (toc - tic))\n",
    "\n",
    "# The loss is a single number, so it is easy to compare the values computed\n",
    "# by the two implementations. The gradient on the other hand is a matrix, so\n",
    "# we use the Frobenius norm to compare them.\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 500: loss 3.000143\n",
      "iteration 100 / 500: loss 2.637392\n"
     ]
    }
   ],
   "source": [
    "# Now implement SGD in LinearSVM.train() function and run it with the code below\n",
    "svm = LinearSVM()\n",
    "tic = time.time()\n",
    "loss_hist = svm.train(X_train, y_train, learning_rate=5e-2, lambda_=0.01,\n",
    "                      num_iters=500, verbose=True, batch_size=20000)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "print('Current loss is %f' % loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function \n",
    "y_train_pred = svm.predict(X_train)\n",
    "# y_test_pred = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#and evaluate the performance on both the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare result with the most common dummy classifier\n",
    "print(classification_report(y_train, [3]*len(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.385378\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(4, X_train_sample.shape[0]) * 0.01 \n",
    "loss, grad = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.000034 analytic: -0.000034, relative error: 6.401608e-08\n",
      "numerical: 0.000011 analytic: 0.000011, relative error: 1.600514e-07\n",
      "numerical: 0.000042 analytic: 0.000042, relative error: 2.248464e-08\n",
      "numerical: 0.000003 analytic: 0.000003, relative error: 3.905276e-06\n",
      "numerical: 0.000008 analytic: 0.000008, relative error: 4.671906e-07\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "f = lambda w: softmax_loss_naive(w, X_train_sample, y_train_sample, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 1.385390e+00 computed in 3.733446s\n",
      "vectorized loss: 1.385390e+00 computed in 0.544031s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_train_sample, y_train_sample, 0.00001)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 500: loss 1.386423\n",
      "iteration 100 / 500: loss 1.364324\n",
      "iteration 200 / 500: loss 1.346996\n",
      "iteration 300 / 500: loss 1.334282\n",
      "iteration 400 / 500: loss 1.324885\n",
      "That took 385.166425s\n",
      "Current loss is 1.315625\n"
     ]
    }
   ],
   "source": [
    "sm = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = sm.train(X_train, y_train, learning_rate=5e-2, lambda_=0.01,\n",
    "                      num_iters=500, verbose=True, batch_size=20000)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "print('Current loss is %f' % loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1170810f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAHuCAYAAADJMutoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxPAmSZLUI4Y3SZKkHjG8SZIk9cj/AT3viqpJsyxnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11863a080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write the LinearSVM.predict function \n",
    "y_train_pred = sm.predict(X_train)\n",
    "y_test_pred = sm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compare result with the most common dummy classifier\n",
    "print classification_report(y_test, [3]*len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 3. Kaggle In Class - 50 Баллов\n",
    "\n",
    "Используйте полученные модели для решения контеста. Выберете одну из моделей, реализуйте настройку гиперпараметров и пайплайн для предсказания классов тестовой выборки для сабмита в систему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 4000: loss 1.386402\n",
      "iteration 100 / 4000: loss 1.361907\n",
      "iteration 200 / 4000: loss 1.338017\n",
      "iteration 300 / 4000: loss 1.319885\n",
      "iteration 400 / 4000: loss 1.300979\n",
      "iteration 500 / 4000: loss 1.281267\n",
      "iteration 600 / 4000: loss 1.268371\n",
      "iteration 700 / 4000: loss 1.247306\n",
      "iteration 800 / 4000: loss 1.240064\n",
      "iteration 900 / 4000: loss 1.231736\n",
      "iteration 1000 / 4000: loss 1.217248\n"
     ]
    }
   ],
   "source": [
    "sm = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = sm.train(X_train, y_train, learning_rate=5e-2, lambda_=0.001,\n",
    "                      num_iters=4000, verbose=True, batch_size=3000)\n",
    "\n",
    "toc = time.time()\n",
    "print('That took %fs' % (toc - tic))\n",
    "print('Current loss is %f' % loss_hist[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = sm.predict_probabilities(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23864203,  0.23634072,  0.24145214,  0.28356511],\n",
       "       [ 0.24968898,  0.24952066,  0.24974207,  0.25104828],\n",
       "       [ 0.24815955,  0.24783334,  0.24960804,  0.25439907],\n",
       "       [ 0.24978738,  0.25002157,  0.24995642,  0.25023463],\n",
       "       [ 0.24848803,  0.24537826,  0.247363  ,  0.25877071],\n",
       "       [ 0.24967804,  0.24975982,  0.24996324,  0.2505989 ],\n",
       "       [ 0.24178787,  0.24034338,  0.24392649,  0.27394226],\n",
       "       [ 0.2444358 ,  0.24425013,  0.24791694,  0.26339713],\n",
       "       [ 0.24658137,  0.24563541,  0.24847282,  0.2593104 ],\n",
       "       [ 0.24093279,  0.24082339,  0.24628   ,  0.27196382]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output_to_csv(result_matrix, indices, filename):\n",
    "    df = pd.DataFrame(result_matrix)\n",
    "    df.index = indices\n",
    "    df.index.name = 'ID'\n",
    "    df.columns = ['class_0', 'class_1', 'class_2', 'class_3']\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_to_csv(probs,test_data.index.values, 'output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### 4. Бонусы - 30 Баллов\n",
    "\n",
    "Улучшите результат: \n",
    "\n",
    "- Нормализуйте слова в документах, используйте стоп слова, попробуйте различные стратегии нормализации в TF-IDF\n",
    "- Используйте PCA, Word2Vec для расширения пространства фичей\n",
    "- Придумайте другие стратегии и улучшайте свой результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
